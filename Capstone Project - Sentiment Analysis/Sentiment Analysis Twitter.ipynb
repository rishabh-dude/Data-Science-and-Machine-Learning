{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style = 'color:green'> Capstone Project </span>\n",
    "#### <span style = 'color:blue'> Problem statement : Perform sentiment analysis on Omnicron variant, data fetching directly from twitter</span>\n",
    "**Sentiment analysis is the process of identifying feelings and emotions expressed in words, through ML or AI**\n",
    "\n",
    "**Project Pipeline**\n",
    "\n",
    "Various steps in completing project are\n",
    "\n",
    "- **Import Necessary Dependencies**\n",
    "- **Read and Load the Dataset**\n",
    "- **Exploratory Data Analysis**\n",
    "- **Data Visualization of Target Variables**\n",
    "- **Data Preprocessing**\n",
    "- **Splitting our data into Train and Test Subset**\n",
    "- **Transforming Dataset using TF-IDF Vectorizer**\n",
    "- **Function for Model Evaluation**\n",
    "- **Model Building**\n",
    "- **Conclusion**\n",
    "\n",
    "- Here we have to get dataset directly fetched from twitter in realtime \n",
    "\n",
    "- performing realtime sentimental analysis on realtime data collecting from twitter\n",
    "- objective: perform sentiment analysis on realtime data collected from twitter \n",
    "\n",
    "                      \n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style = 'color:blue'>   API (Application Programm Interface)</span>\n",
    "- Imagine you’re sitting at a table in a restaurant with a menu of choices to order from. The kitchen is the part of the “system” that will prepare your order. What is missing is the critical link to communicate your order to the kitchen and deliver your food back to your table. That’s where the waiter or API comes in. The waiter is the messenger – or API – that takes your request or order and tells the kitchen – the system – what to do. Then the waiter delivers the response back to you; in this case, it is the food.\n",
    "- API's are huge and are used everywhere\n",
    "- In simple words api stands as bridge for one to access the content in one's storage \n",
    "- There are many APIs on the Twitter platform that software developers can engage with, with the ultimate possibility to create fully automated systems which will interact with Twitter. While this feature could benefit companies by drawing insights from Twitter data\n",
    "\n",
    "   **From twitter api it's possible to extract many insights some are**\n",
    "- Tweets: searching, posting, filtering, engagement, streaming etc.\n",
    "- Accounts and users (Beta): account management, user interactions.\n",
    "- Media: uploading and accessing photos, videos and animated GIFs.\n",
    "- Trends: trending topics in a given location.\n",
    "- Geo: information about known places or places near a location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting twitter API keys**\n",
    "- If you don't already have an account, you can login with your normal Twitter credentials \n",
    "\n",
    "\n",
    "- follow the required prompts to create a developer project or click here <a href=\"https://dev.twitter.com/apps\" title=\"Twitter\">Click here</a>\n",
    "- Requesting the API key and secret via the Developer Portal causes Twitter to produce the following three things:\n",
    "1. API key (this is your 'consumer key')\n",
    "2. API secret key (this is your 'consumer secret')\n",
    "3. Bearer token\n",
    "- Next, visit the 'Authentication Tokens' area of the Developer Portal and generate an 'Access token & secret'. This will provide you with the following two items:\n",
    "1. Access token (this is your 'token key')\n",
    "2. Access token secret (this is your 'token secret')\n",
    "\n",
    "\n",
    "**Expected output**\n",
    "- the data fetched from twitter should undergo EDA for analyzing, cleaning, handling, manupulation, visualization..,etc\n",
    "- final output should show the sentiment of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  *Some tips to consider*\n",
    "\n",
    "- Machines can learn in every possible way so its always better to think out of the box\n",
    "    \n",
    "- Perform eda as diverse as possible and in contineous manner\n",
    "    \n",
    "- Try configuring with diffrent models to know how each model is diffrent with other ones \n",
    "    \n",
    "- Donot try to involve unneccesory codes and useless algorithms for dataset which just increases complexity\n",
    "    \n",
    "- Approaching problem statement in n number of ways helps us to find best one possible\n",
    "    \n",
    "- It's easier for one to understnd and manupulate if we have models as simple as possible \n",
    "    \n",
    "- When we have multiple models we can have multiple judgements based on models and their efficiencies\n",
    "    \n",
    "- Tuning helps increasing accuracy :)\n",
    "    \n",
    "- Have an idea of time consumed by the model, its better to have a model whose time management is good\n",
    "    \n",
    "- Spend good amount of time on analyzing dataset and draw as much insights as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tweepy is importantlibrary we will using to fetch data from twitter by api\n",
    "\n",
    "\n",
    "For more on tweepy documentation please click here <a href=\"https://docs.tweepy.org/en/stable/getting_started.html#hello-tweepy\" title=\"Tweepy\">Click here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tweepy\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "import logging\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from credentials import * # pip install --upgrade credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder,LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # pip install nltk --upgrade\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem import LancasterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from textblob import TextBlob,Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "leammatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ToktokTokenizer()\n",
    "stopwords_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning the keys\n",
    "#consumer_key = ''\n",
    "#consumer_secret = ''\n",
    "#access_token = ''\n",
    "#access_token_secret = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "#auth.set_access_token(access_token, access_token_secret)\n",
    "#api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search_query = \"'ref''omicron'-filter:retweets AND -filter:replies AND -filter:links\"\n",
    "#no_of_tweets = 100\n",
    "\n",
    "#try:\n",
    "#The number of tweets we want to retrieved from the search\n",
    "#    tweets = api.search_tweets(q=search_query, lang=\"en\", count=no_of_tweets, tweet_mode ='extended')\n",
    "    \n",
    "#Pulling Some attributes from the tweet\n",
    "#    attributes_container = [[tweet.full_text, tweet.favorite_count, tweet.created.at, tweet.retweet_count] for tweet in tweets]\n",
    "\n",
    "#Creation of column list to rename the columns in the dataframe\n",
    "#   columns = [\"tweets\", \"likes\", \"time\", \"retweet_count\"]\n",
    "    \n",
    "#Creation of Dataframe\n",
    "#    tweets_df = pd.DataFrame(attributes_container, columns=columns)\n",
    "#except BaseException as e:\n",
    "#    print('Status Failed On,',str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_df.to_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Omicron_data.csv') # Using the csv file provided by Skillovilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\S+\\.com\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespaces\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)  # Remove non-alphabetic characters\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    return text\n",
    "\n",
    "df['tweets'] = df['tweets'].apply(preprocess_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleStemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "df['stem_tweets'] = df['tweets'].apply(simpleStemmer)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleLemmatization(text):\n",
    "    lemma = nltk.stem.WordNetLemmatizer()\n",
    "    text = ' '.join([lemma.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "df['lemm_tweets'] = df['tweets'].apply(simpleLemmatization)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text,is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "df['final_tweets'] = df['tweets'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = df.copy()\n",
    "sent_df[\"sentiment_score\"] = ''\n",
    "sent_df[\"Negative\"] = ''\n",
    "sent_df[\"Neutral\"] = ''\n",
    "sent_df[\"Positive\"] = ''\n",
    "sent_df.drop('tweets', axis=1, inplace=True)\n",
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(tweets):\n",
    "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "    sentence_text = unicodedata.normalize('NFKD', tweets)\n",
    "    try:\n",
    "        sentence_sentiment = sentiment_analyzer.polarity_scores(sentence_text)\n",
    "        return sentence_sentiment\n",
    "    except TypeError as e:\n",
    "        logging.error(f\"Error analyzing sentiment for text '{tweets}': {e}\")\n",
    "        return None\n",
    "\n",
    "sent_df['sentiment_score'] = sent_df['final_tweets'].apply(lambda x: analyze_sentiment(x)['compound'] if analyze_sentiment(x) else None)\n",
    "sent_df['Negative'] = sent_df['final_tweets'].apply(lambda x: analyze_sentiment(x)['neg'] if analyze_sentiment(x) else None)\n",
    "sent_df['Neutral'] = sent_df['final_tweets'].apply(lambda x: analyze_sentiment(x)['neu'] if analyze_sentiment(x) else None)\n",
    "sent_df['Positive'] = sent_df['final_tweets'].apply(lambda x: analyze_sentiment(x)['pos'] if analyze_sentiment(x) else None)\n",
    "\n",
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = [ tweet for index, tweet in enumerate(sent_df['final_tweets']) if sent_df['sentiment_score'][index] > 0]\n",
    "neutral_tweets = [ tweet for index, tweet in enumerate(sent_df['final_tweets']) if sent_df['sentiment_score'][index] == 0]\n",
    "negitive_tweets = [ tweet for index, tweet in enumerate(sent_df['final_tweets']) if sent_df['sentiment_score'][index] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of positive tweets: {}%\".format(len(positive_tweets)*100/len(sent_df['final_tweets'])))\n",
    "print(\"Percentage of neutral tweets: {}%\".format(len(neutral_tweets)*100/len(sent_df['final_tweets'])))\n",
    "print(\"Percentage de negative tweets: {}%\".format(len(negitive_tweets)*100/len(sent_df['final_tweets'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "\n",
    "According to our analysis by fetching live data from twitter, we got to know that the sentiment of people on Omicron virus is\n",
    "\n",
    "Approx: 39% of tweets are positive\n",
    "Approx: 23% of tweets are neutral\n",
    "Approx: 38% of tweets are negitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the Index Coulumn\n",
    "sent_df.drop(sent_df.iloc[:,:1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "sent_df['datetime'] = pd.to_datetime(sent_df['time'])\n",
    "sent_df['day'] = sent_df['datetime'].dt.day\n",
    "sent_df['month'] = sent_df['datetime'].dt.month\n",
    "sent_df['year'] = sent_df['datetime'].dt.year\n",
    "\n",
    "sent_df.drop('time', axis=1, inplace=True)\n",
    "\n",
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataprep\n",
    "from dataprep.eda import create_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalised train and test reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_reviews = sent_df.final_tweets[:4900]\n",
    "print(norm_train_reviews[1])\n",
    "\n",
    "norm_test_reviews = sent_df.final_tweets[4900:]\n",
    "print(norm_test_reviews[6997])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_reviews = norm_train_reviews.dropna()\n",
    "norm_test_reviews = norm_test_reviews.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=0.01,max_df=1.0,binary=False,ngram_range=(1,3))\n",
    "cv_train_reviews = cv.fit_transform(norm_train_reviews)\n",
    "cv_test_reviews = cv.transform(norm_test_reviews)\n",
    "\n",
    "print('BOW_cv_train:',cv_train_reviews.shape)\n",
    "print('BOW_cv_test:',cv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF Vectorizer; Term frequency and inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(min_df=0.01,max_df=1.0,use_idf=True,ngram_range=(1,3))\n",
    "tv_train_reviews = tv.fit_transform(norm_train_reviews)\n",
    "tv_test_reviews=tv.transform(norm_test_reviews)\n",
    "\n",
    "\n",
    "print('Tfidf_train:',tv_train_reviews.shape)\n",
    "print('Tfidf_test:',tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting graph to visualize the most common words in tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = lambda x:word_tokenize(x)\n",
    "df['tokenize'] = df['final_tweets'].apply(tok)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools,collections\n",
    "\n",
    "new_tokenize = df['tokenize']\n",
    "all_words = list(itertools.chain(*new_tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = collections.Counter(all_words)\n",
    "count_frequency = counts.most_common(200)\n",
    "\n",
    "clean_tweets = pd.DataFrame(counts.most_common(200),columns=['words', 'count'])\n",
    "clean_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "clean_tweets[:25].sort_values(by='count').plot.barh(x='words',y='count',ax=ax,color='maroon')\n",
    "ax.set_title(\"Common Words Found in Tweets (Including All Words)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorize tweets as Positive, Negative or Neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAN = sent_df['sentiment_score'].apply(lambda x: \"Positive\" if x>=0.05 else (\"Negative\" if x<= -0.05 else \"Neutral\"))\n",
    "new_review = sent_df['final_tweets']\n",
    "new_review = new_review.tolist()\n",
    "SAN = SAN.tolist()\n",
    "\n",
    "dict = {'final_tweets':new_review, 'sentiment_score':SAN}\n",
    "pnn = pd.DataFrame(dict)\n",
    "pnn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the most common words used positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "positive_text=norm_train_reviews[1]\n",
    "WC=WordCloud(width=1000,height=500,max_words=500,min_font_size=5)\n",
    "positive_words=WC.generate(positive_text)\n",
    "plt.imshow(positive_words,interpolation='bilinear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "negative_text=norm_train_reviews[8]\n",
    "WC=WordCloud(width=1000,height=500,max_words=500,min_font_size=5)\n",
    "negative_words=WC.generate(negative_text)\n",
    "plt.imshow(negative_words,interpolation='bilinear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the numerical data to categorical - Positive and Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df['Sentiment'] = sent_df['sentiment_score'].apply(lambda x: \"Positive\" if x>=0 else \"Negative\")\n",
    "\n",
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "sent_df['label'] = lb.fit_transform(sent_df['Sentiment'])\n",
    "\n",
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Test & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df = sent_df.copy()\n",
    "ml_df.drop(['retweet_count', 'stem_tweets', 'lemm_tweets', 'Negative', 'Neutral', 'Positive'], axis=1, inplace=True)\n",
    "\n",
    "ml_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ml_df.label[:4900]\n",
    "test = ml_df.label[4900:]\n",
    "\n",
    "ml_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
    "lr_bow = lr.fit(cv_train_reviews,train) #From Bag of Words\n",
    "print(lr_bow)\n",
    "\n",
    "lr_tfidf = lr.fit(tv_train_reviews,train) #TFIDF \n",
    "print(lr_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_bow_predict = lr.predict(cv_test_reviews)\n",
    "print(lr_bow_predict)\n",
    "lr_tfidf_predict = lr.predict(tv_test_reviews)\n",
    "print(lr_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr_bow_predict.shape)\n",
    "print(lr_tfidf_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_test_reviews[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ml_df['final_tweets'][4900:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'text':text,'test':test, 'bow':lr_bow_predict, 'tfidf': lr_tfidf_predict}\n",
    "df1 = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['test']!=df1['bow']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_bow_score = accuracy_score(test,lr_bow_predict)\n",
    "print(lr_bow_score)\n",
    "lr_tfidf_score = accuracy_score(test,lr_tfidf_predict)\n",
    "print(lr_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_bow_report=classification_report(test,lr_bow_predict,target_names=['Negative','Positive'])\n",
    "print(lr_bow_report)\n",
    "lr_tfidf_report = classification_report(test,lr_tfidf_predict,target_names=['Negative','Positive'])\n",
    "print(lr_tfidf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_bow = confusion_matrix(test,lr_bow_predict,labels=[0,1])\n",
    "print(cm_bow)\n",
    "cm_tfidf = confusion_matrix(test,lr_tfidf_predict,labels=[0,1])\n",
    "print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
    "lr_bow = lr.fit(cv_train_reviews,train) #From Bag of Words\n",
    "print(lr_bow)\n",
    "\n",
    "lr_tfidf = lr.fit(tv_train_reviews,train) #TFIDF \n",
    "print(lr_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear support vector machines for bag of words and tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm = svm.SVC(kernel = 'linear', random_state = 0, C=1.0)\n",
    "svm_bow = svm.fit(cv_train_reviews,train)\n",
    "print(svm_bow)\n",
    "svm_tfidf = svm.fit(tv_train_reviews,train)\n",
    "print(svm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_bow_predict = svm.predict(cv_test_reviews)\n",
    "print(svm_bow_predict)\n",
    "svm_tfidf_predict = svm.predict(tv_test_reviews)\n",
    "print(svm_tfidf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_bow_score = accuracy_score(test,svm_bow_predict)\n",
    "print(svm_bow_score)\n",
    "svm_tfidf_score = accuracy_score(test,svm_tfidf_predict)\n",
    "print(svm_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_bow_report = classification_report(test,svm_bow_predict,target_names=['Positive','Negative'])\n",
    "print(svm_bow_report)\n",
    "svm_tfidf_report = classification_report(test,svm_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(svm_tfidf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_bow = confusion_matrix(test,svm_bow_predict,labels=[1,0])\n",
    "print(cm_bow)\n",
    "cm_tfidf = confusion_matrix(test,svm_tfidf_predict,labels=[1,0])\n",
    "print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_bow = knn.fit(cv_train_reviews,train)\n",
    "print(knn_bow)\n",
    "knn_tfidf = knn.fit(tv_train_reviews,train)\n",
    "print(knn_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_bow_predict = knn.predict(cv_test_reviews)\n",
    "print(knn_bow_predict)\n",
    "knn_tfidf_predict = knn.predict(tv_test_reviews)\n",
    "print(knn_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_bow_score = accuracy_score(test,knn_bow_predict)\n",
    "print(knn_bow_score)\n",
    "knn_tfidf_score = accuracy_score(test,knn_tfidf_predict)\n",
    "print(knn_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_bow_report = classification_report(test,knn_bow_predict,target_names=['Positive','Negative'])\n",
    "print(knn_bow_report)\n",
    "knn_tfidf_report = classification_report(test,knn_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(knn_tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_bow = confusion_matrix(test,knn_bow_predict,labels=[0,1])\n",
    "print(cm_bow)\n",
    "cm_tfidf = confusion_matrix(test,knn_tfidf_predict,labels=[0,1])\n",
    "print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier (criterion = 'entropy',max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_bow = dtc.fit(cv_train_reviews,train)\n",
    "print(dtc_bow)\n",
    "dtc_tfidf = dtc.fit(tv_train_reviews,train)\n",
    "print(dtc_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_bow_predict = dtc.predict(cv_test_reviews)\n",
    "print(dtc_bow_predict)\n",
    "dtc_tfidf_predict = dtc.predict(tv_test_reviews)\n",
    "print(dtc_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_bow_score = accuracy_score(test,dtc_bow_predict)\n",
    "print(dtc_bow_score)\n",
    "dtc_tfidf_score = accuracy_score(test,dtc_tfidf_predict)\n",
    "print(dtc_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_bow_report = classification_report(test,dtc_bow_predict,target_names=['Positive','Negative'])\n",
    "print(dtc_bow_report)\n",
    "dtc_tfidf_report = classification_report(test,dtc_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(dtc_tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_bow = confusion_matrix(test,dtc_bow_predict,labels=[0,1])\n",
    "print(cm_bow)\n",
    "cm_tfidf = confusion_matrix(test,dtc_tfidf_predict,labels=[0,1])\n",
    "print(cm_tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
